idea:
  title: Are there any finetuning proof datasets currently?
  domain: machine_learning
  hypothesis: 'Some datasets are more resistant to performance gains from model finetuning
    due to minimal train/test overlap or lack of subdistribution amplification. This
    research will identify which datasets, if any, are finetuning resistant or finetuning
    proof.

    '
  background:
    description: Most datasets get easier if models finetune on them, merely because
      of train/test overlap or because we amplify a subdistribution within the model
      that makes the model more confident in its already-known correct answers. Which
      datasets are most finetuning resistant? Are any of them finetuning proof?
  metadata:
    source: IdeaHub
    source_url: https://hypogenic.ai/ideahub/idea/XMu5uZXUuUig7I2khHhL
    idea_id: are_there_any_finetuning_proof_20251130_191857_86eeb0b1
    created_at: '2025-11-30T19:18:57.971286'
    status: submitted
    github_repo_name: finetuning-proof-datasets-c76e
    github_repo_url: https://github.com/ChicagoHAI/finetuning-proof-datasets-c76e
