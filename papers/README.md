# Downloaded Papers

1. [Inverse Scaling: When Bigger Isn't Better](2306.09479_inverse_scaling.pdf)
   - Authors: McKensie et al.
   - Year: 2023
   - arXiv: 2306.09479
   - Why relevant: Identifies datasets where larger models/training degrade performance.

2. [On the Measure of Intelligence](1911.01547_measure_of_intelligence.pdf)
   - Authors: Fran√ßois Chollet
   - Year: 2019
   - arXiv: 1911.01547
   - Why relevant: Introduces ARC, a benchmark designed to be resistant to memorization/finetuning.

3. [Fine-Tuning can Distort Pretrained Features and Underperform Out-of-Distribution](2202.10054_ft_distorts_features.pdf)
   - Authors: Kumar et al.
   - Year: 2022
   - arXiv: 2202.10054
   - Why relevant: Theoretical and empirical evidence of finetuning failure modes (distortion).

4. [The Curse of Recursion: Training on Generated Data Makes Models Forget](2305.17493_curse_of_recursion.pdf)
   - Authors: Shumailov et al.
   - Year: 2023
   - arXiv: 2305.17493
   - Why relevant: Shows how finetuning on model-generated data leads to collapse/resistance.
